{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def __init__(self):\n",
    "        self.stop_words = \"'d\\n'll\\n'm\\n're\\n's\\n't\\n've\\nZT\\nZZ\\na\\na's\\nable\\nabout\\nabove\\nabst\\naccordance\\naccording\\naccordingly\\nacross\\nact\\nactually\\nadded\\nadj\\nadopted\\naffected\\naffecting\\naffects\\nafter\\nafterwards\\nagain\\nagainst\\nah\\nain't\\nall\\nallow\\nallows\\nalmost\\nalone\\nalong\\nalready\\nalso\\nalthough\\nalways\\nam\\namong\\namongst\\nan\\nand\\nannounce\\nanother\\nany\\nanybody\\nanyhow\\nanymore\\nanyone\\nanything\\nanyway\\nanyways\\nanywhere\\napart\\napparently\\nappear\\nappreciate\\nappropriate\\napproximately\\nare\\narea\\nareas\\naren\\naren't\\narent\\narise\\naround\\nas\\naside\\nask\\nasked\\nasking\\nasks\\nassociated\\nat\\nauth\\navailable\\naway\\nawfully\\nb\\nback\\nbacked\\nbacking\\nbacks\\nbe\\nbecame\\nbecause\\nbecome\\nbecomes\\nbecoming\\nbeen\\nbefore\\nbeforehand\\nbegan\\nbegin\\nbeginning\\nbeginnings\\nbegins\\nbehind\\nbeing\\nbeings\\nbelieve\\nbelow\\nbeside\\nbesides\\nbest\\nbetter\\nbetween\\nbeyond\\nbig\\nbiol\\nboth\\nbrief\\nbriefly\\nbut\\nby\\nc\\nc'mon\\nc's\\nca\\ncame\\ncan\\ncan't\\ncannot\\ncant\\ncase\\ncases\\ncause\\ncauses\\ncertain\\ncertainly\\nchanges\\nclear\\nclearly\\nco\\ncom\\ncome\\ncomes\\nconcerning\\nconsequently\\nconsider\\nconsidering\\ncontain\\ncontaining\\ncontains\\ncorresponding\\ncould\\ncouldn't\\ncouldnt\\ncourse\\ncurrently\\nd\\ndate\\ndefinitely\\ndescribe\\ndescribed\\ndespite\\ndid\\ndidn't\\ndiffer\\ndifferent\\ndifferently\\ndiscuss\\ndo\\ndoes\\ndoesn't\\ndoing\\ndon't\\ndone\\ndown\\ndowned\\ndowning\\ndowns\\ndownwards\\ndue\\nduring\\ne\\neach\\nearly\\ned\\nedu\\neffect\\neg\\neight\\neighty\\neither\\nelse\\nelsewhere\\nend\\nended\\nending\\nends\\nenough\\nentirely\\nespecially\\net\\net-al\\netc\\neven\\nevenly\\never\\nevery\\neverybody\\neveryone\\neverything\\neverywhere\\nex\\nexactly\\nexample\\nexcept\\nf\\nface\\nfaces\\nfact\\nfacts\\nfar\\nfelt\\nfew\\nff\\nfifth\\nfind\\nfinds\\nfirst\\nfive\\nfix\\nfollowed\\nfollowing\\nfollows\\nfor\\nformer\\nformerly\\nforth\\nfound\\nfour\\nfrom\\nfull\\nfully\\nfurther\\nfurthered\\nfurthering\\nfurthermore\\nfurthers\\ng\\ngave\\ngeneral\\ngenerally\\nget\\ngets\\ngetting\\ngive\\ngiven\\ngives\\ngiving\\ngo\\ngoes\\ngoing\\ngone\\ngood\\ngoods\\ngot\\ngotten\\ngreat\\ngreater\\ngreatest\\ngreetings\\ngroup\\ngrouped\\ngrouping\\ngroups\\nh\\nhad\\nhadn't\\nhappens\\nhardly\\nhas\\nhasn't\\nhave\\nhaven't\\nhaving\\nhe\\nhe's\\nhed\\nhello\\nhelp\\nhence\\nher\\nhere\\nhere's\\nhereafter\\nhereby\\nherein\\nheres\\nhereupon\\nhers\\nherself\\nhes\\nhi\\nhid\\nhigh\\nhigher\\nhighest\\nhim\\nhimself\\nhis\\nhither\\nhome\\nhopefully\\nhow\\nhowbeit\\nhowever\\nhundred\\ni\\ni'd\\ni'll\\ni'm\\ni've\\nid\\nie\\nif\\nignored\\nim\\nimmediate\\nimmediately\\nimportance\\nimportant\\nin\\ninasmuch\\ninc\\ninclude\\nindeed\\nindex\\nindicate\\nindicated\\nindicates\\ninformation\\ninner\\ninsofar\\ninstead\\ninterest\\ninterested\\ninteresting\\ninterests\\ninto\\ninvention\\ninward\\nis\\nisn't\\nit\\nit'd\\nit'll\\nit's\\nitd\\nits\\nitself\\nj\\njust\\nk\\nkeep\\nkeeps\\nkept\\nkeys\\nkg\\nkind\\nkm\\nknew\\nknow\\nknown\\nknows\\nl\\nlarge\\nlargely\\nlast\\nlately\\nlater\\nlatest\\nlatter\\nlatterly\\nleast\\nless\\nlest\\nlet\\nlet's\\nlets\\nlike\\nliked\\nlikely\\nline\\nlittle\\nlong\\nlonger\\nlongest\\nlook\\nlooking\\nlooks\\nltd\\nm\\nmade\\nmainly\\nmake\\nmakes\\nmaking\\nman\\nmany\\nmay\\nmaybe\\nme\\nmean\\nmeans\\nmeantime\\nmeanwhile\\nmember\\nmembers\\nmen\\nmerely\\nmg\\nmight\\nmillion\\nmiss\\nml\\nmore\\nmoreover\\nmost\\nmostly\\nmr\\nmrs\\nmuch\\nmug\\nmust\\nmy\\nmyself\\nn\\nn't\\nna\\nname\\nnamely\\nnay\\nnd\\nnear\\nnearly\\nnecessarily\\nnecessary\\nneed\\nneeded\\nneeding\\nneeds\\nneither\\nnever\\nnevertheless\\nnew\\nnewer\\nnewest\\nnext\\nnine\\nninety\\nno\\nnobody\\nnon\\nnone\\nnonetheless\\nnoone\\nnor\\nnormally\\nnos\\nnot\\nnoted\\nnothing\\nnovel\\nnow\\nnowhere\\nnumber\\nnumbers\\no\\nobtain\\nobtained\\nobviously\\nof\\noff\\noften\\noh\\nok\\nokay\\nold\\nolder\\noldest\\nomitted\\non\\nonce\\none\\nones\\nonly\\nonto\\nopen\\nopened\\nopening\\nopens\\nor\\nord\\norder\\nordered\\nordering\\norders\\nother\\nothers\\notherwise\\nought\\nour\\nours\\nourselves\\nout\\noutside\\nover\\noverall\\nowing\\nown\\np\\npage\\npages\\npart\\nparted\\nparticular\\nparticularly\\nparting\\nparts\\npast\\nper\\nperhaps\\nplace\\nplaced\\nplaces\\nplease\\nplus\\npoint\\npointed\\npointing\\npoints\\npoorly\\npossible\\npossibly\\npotentially\\npp\\npredominantly\\npresent\\npresented\\npresenting\\npresents\\npresumably\\npreviously\\nprimarily\\nprobably\\nproblem\\nproblems\\npromptly\\nproud\\nprovides\\nput\\nputs\\nq\\nque\\nquickly\\nquite\\nqv\\nr\\nran\\nrather\\nrd\\nre\\nreadily\\nreally\\nreasonably\\nrecent\\nrecently\\nref\\nrefs\\nregarding\\nregardless\\nregards\\nrelated\\nrelatively\\nresearch\\nrespectively\\nresulted\\nresulting\\nresults\\nright\\nroom\\nrooms\\nrun\\ns\\nsaid\\nsame\\nsaw\\nsay\\nsaying\\nsays\\nsec\\nsecond\\nsecondly\\nseconds\\nsection\\nsee\\nseeing\\nseem\\nseemed\\nseeming\\nseems\\nseen\\nsees\\nself\\nselves\\nsensible\\nsent\\nserious\\nseriously\\nseven\\nseveral\\nshall\\nshe\\nshe'll\\nshed\\nshes\\nshould\\nshouldn't\\nshow\\nshowed\\nshowing\\nshown\\nshowns\\nshows\\nside\\nsides\\nsignificant\\nsignificantly\\nsimilar\\nsimilarly\\nsince\\nsix\\nslightly\\nsmall\\nsmaller\\nsmallest\\nso\\nsome\\nsomebody\\nsomehow\\nsomeone\\nsomethan\\nsomething\\nsometime\\nsometimes\\nsomewhat\\nsomewhere\\nsoon\\nsorry\\nspecifically\\nspecified\\nspecify\\nspecifying\\nstate\\nstates\\nstill\\nstop\\nstrongly\\nsub\\nsubstantially\\nsuccessfully\\nsuch\\nsufficiently\\nsuggest\\nsup\\nsure\\nt\\nt's\\ntake\\ntaken\\ntaking\\ntell\\ntends\\nth\\nthan\\nthank\\nthanks\\nthanx\\nthat\\nthat'll\\nthat's\\nthat've\\nthats\\nthe\\ntheir\\ntheirs\\nthem\\nthemselves\\nthen\\nthence\\nthere\\nthere'll\\nthere's\\nthere've\\nthereafter\\nthereby\\nthered\\ntherefore\\ntherein\\nthereof\\ntherere\\ntheres\\nthereto\\nthereupon\\nthese\\nthey\\nthey'd\\nthey'll\\nthey're\\nthey've\\ntheyd\\ntheyre\\nthing\\nthings\\nthink\\nthinks\\nthird\\nthis\\nthorough\\nthoroughly\\nthose\\nthou\\nthough\\nthoughh\\nthought\\nthoughts\\nthousand\\nthree\\nthroug\\nthrough\\nthroughout\\nthru\\nthus\\ntil\\ntip\\nto\\ntoday\\ntogether\\ntoo\\ntook\\ntoward\\ntowards\\ntried\\ntries\\ntruly\\ntry\\ntrying\\nts\\nturn\\nturned\\nturning\\nturns\\ntwice\\ntwo\\nu\\nun\\nunder\\nunfortunately\\nunless\\nunlike\\nunlikely\\nuntil\\nunto\\nup\\nupon\\nups\\nus\\nuse\\nused\\nuseful\\nusefully\\nusefulness\\nuses\\nusing\\nusually\\nuucp\\nv\\nvalue\\nvarious\\nvery\\nvia\\nviz\\nvol\\nvols\\nvs\\nw\\nwant\\nwanted\\nwanting\\nwants\\nwas\\nwasn't\\nway\\nways\\nwe\\nwe'd\\nwe'll\\nwe're\\nwe've\\nwed\\nwelcome\\nwell\\nwells\\nwent\\nwere\\nweren't\\nwhat\\nwhat'll\\nwhat's\\nwhatever\\nwhats\\nwhen\\nwhence\\nwhenever\\nwhere\\nwhere's\\nwhereafter\\nwhereas\\nwhereby\\nwherein\\nwheres\\nwhereupon\\nwherever\\nwhether\\nwhich\\nwhile\\nwhim\\nwhither\\nwho\\nwho'll\\nwho's\\nwhod\\nwhoever\\nwhole\\nwhom\\nwhomever\\nwhos\\nwhose\\nwhy\\nwidely\\nwill\\nwilling\\nwish\\nwith\\nwithin\\nwithout\\nwon't\\nwonder\\nwords\\nwork\\nworked\\nworking\\nworks\\nworld\\nwould\\nwouldn't\\nwww\\nx\\ny\\nyear\\nyears\\nyes\\nyet\\nyou\\nyou'd\\nyou'll\\nyou're\\nyou've\\nyoud\\nyoung\\nyounger\\nyoungest\\nyour\\nyoure\\nyours\\nyourself\\nyourselves\\nz\\nzero\\nzt\\nzz\"\n",
    "\n",
    "    def classification(self,text):\n",
    "        import math\n",
    "        class TfIdf:\n",
    "            def __init__(self, idf_dic, default_idf, word_list,words):\n",
    "                \"\"\"\n",
    "                TF-IDF类的构造函数\n",
    "                :param idf_dic: 训练好的idf词典\n",
    "                :param default_idf: 默认idf值\n",
    "                :param word_list: 经处理的(过滤停用词)待提取关键词文本列表\n",
    "                :param keyword_num: 关键词数量\n",
    "                \"\"\"\n",
    "                self.word_list = word_list\n",
    "                self.idf_dic = idf_dic\n",
    "                self.default_idf = default_idf\n",
    "                self.words = words\n",
    "                self.tf_dic = self.get_tf_dic()\n",
    "\n",
    "\n",
    "\n",
    "            def get_tf_dic(self):\n",
    "                \"\"\"\n",
    "                计算输入的文本列表所有词的tf值\n",
    "                :return:\n",
    "                \"\"\"\n",
    "\n",
    "                tf_dic = {}\n",
    "                for word in self.word_list:\n",
    "                    tf_dic[word] = tf_dic.get(word, 0) + 1.0\n",
    "\n",
    "\n",
    "                for k, v in tf_dic.items():\n",
    "                    tf_dic[k] = float(v) / len(self.word_list)\n",
    "\n",
    "                return tf_dic\n",
    "\n",
    "            def get_tfidf(self):\n",
    "                \"\"\"\n",
    "                计算输入的文本列表所有词的tf-idf值：tf * idf\n",
    "                :return: 打印前keyword_num个待提取关键词文本的提取关键词结果\n",
    "                \"\"\"\n",
    "                tfidf_dic = {}\n",
    "                for word in self.word_list:\n",
    "                    idf = self.idf_dic.get(word, self.default_idf)\n",
    "                    tf = self.tf_dic.get(word, 0.0)\n",
    "\n",
    "                    tfidf = tf * idf\n",
    "                    tfidf_dic[word] = tfidf\n",
    "\n",
    "                tfidf_vec = []\n",
    "                for word in self.words:\n",
    "                    if tfidf_dic.get(word,-1)!=-1:\n",
    "                        tfidf_vec.append(tfidf_dic[word])\n",
    "                    else:\n",
    "                        tfidf_vec.append(round(0,5))\n",
    "\n",
    "\n",
    "                return tfidf_vec\n",
    "        def train_idf(doc_list):\n",
    "            \"\"\"\n",
    "                从整体语料库中学习到各个词的idf值\n",
    "                param doc_list:文章列表\n",
    "            \"\"\"\n",
    "            idf_dic = {}\n",
    "\n",
    "            tf_count = len(doc_list)\n",
    "\n",
    "            #每个词出现的文档数\n",
    "            for doc in doc_list:\n",
    "                for word in set(doc):\n",
    "                    idf_dic[word] = idf_dic.get(word,0.0) +1.0\n",
    "\n",
    "            #按照公式转化为idf值，分母加1进行平滑处理\n",
    "            for k,v in idf_dic.items():\n",
    "                idf_dic[k] = math.log(tf_count/(1.0+v))\n",
    "            return idf_dic\n",
    "    \n",
    "        def get_final_split_result(text=None):\n",
    "            \"\"\"\n",
    "                调用上面三个函数，将输入变成分好词以空格进行分割的字符串\n",
    "                param text:线上使用时直接将text传入，不需要进行读取文件\n",
    "            \"\"\"\n",
    "            dealed_texts = get_dealed_text(text)  \n",
    "            stopwords = get_stopword(self.stop_words)\n",
    "            result = []\n",
    "\n",
    "            for dealed_text in dealed_texts:\n",
    "                words = split_word(dealed_text)\n",
    "                tmp = \"\"\n",
    "                for word in words:\n",
    "                    if word not in stopwords:\n",
    "                        tmp+=\" \"+word\n",
    "                result.append(tmp)\n",
    "            return result\n",
    "        def get_dealed_text(ori_text):\n",
    "            \"\"\"\n",
    "                从输入数据获取各个短文\n",
    "                return list\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            dealed_text = []\n",
    "            for text in ori_text.split(\"\\n\"):\n",
    "                if text!=\"\":\n",
    "                    dealed_text.append(text)\n",
    "            return dealed_text\n",
    "\n",
    "        def get_stopword(stopword):\n",
    "            \"\"\"\n",
    "                获取停用词表\n",
    "                return list\n",
    "            \"\"\"\n",
    "            stopword_list = stopword.splitlines()\n",
    "\n",
    "            return stopword_list\n",
    "\n",
    "\n",
    "        def split_word(parase):\n",
    "            \"\"\"\n",
    "                初步切分\n",
    "                return list\n",
    "            \"\"\"\n",
    "            #处理标点符号符号\n",
    "            parase = parase.replace(\".\",\" \").replace(\"(\",\" \").replace(\",\",\" \")\n",
    "            parase = parase.replace(\"(\",\"\").replace(\")\",\"\")\n",
    "            #TODO：地名处理\n",
    "            #年代等数字处理\n",
    "\n",
    "\n",
    "            tmp = parase.split(\" \")\n",
    "            return tmp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def get_doc_list(docs):\n",
    "            \"\"\"\n",
    "                将分词、去除停用词后的字符串转化成list\n",
    "            \"\"\"\n",
    "            doc_list = []\n",
    "            for doc in docs:\n",
    "                doc_list.append(doc.split())\n",
    "            return doc_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def cos_sim(vector_a, vector_b):\n",
    "            \"\"\"\n",
    "            计算两个向量之间的余弦相似度\n",
    "            :param vector_a: 向量 a \n",
    "            :param vector_b: 向量 b\n",
    "            :return: sim\n",
    "            \"\"\"\n",
    "            num = 0.0\n",
    "            denom = 0.0\n",
    "            tmp1 = 0.0\n",
    "            tmp2 = 0.0\n",
    "            for i in range(len(vector_a)):\n",
    "                num = num+vector_a[i]*vector_b[i]\n",
    "                tmp1 = vector_a[i]*vector_a[i]+tmp1\n",
    "                tmp2 = vector_b[i]*vector_b[i]+tmp2\n",
    "\n",
    "            denom = math.sqrt(tmp1)*math.sqrt(tmp2)\n",
    "            cos_sim = float(num) / denom\n",
    "            return cos_sim\n",
    "\n",
    "\n",
    "\n",
    "        def get_sim(input_data):\n",
    "            \"\"\"\n",
    "                获得文本之间的相似度\n",
    "            \"\"\"\n",
    "            result = []\n",
    "            for i in input_data:\n",
    "                tmp = []\n",
    "                for j in input_data:\n",
    "                    tmp.append(cos_sim(i,j))\n",
    "                result.append(tmp)\n",
    "            return result\n",
    "\n",
    "        def get_max_sim(input_data):\n",
    "            result = []\n",
    "\n",
    "            for i,data in enumerate(input_data):\n",
    "                max_sim = 0\n",
    "                max_sim_index = 0\n",
    "                for j,sim in enumerate(data):\n",
    "                    if i!=j:\n",
    "                        if sim>=max_sim:\n",
    "                            max_sim = sim\n",
    "                            max_sim_index = j\n",
    "                result.append((i,max_sim_index,max_sim))\n",
    "            return result\n",
    "\n",
    "        def get_result(input_data,threshold=0.01):\n",
    "            tmp_dict = {}\n",
    "            type_index = 0\n",
    "\n",
    "            for i in input_data:\n",
    "                if i[2]>=threshold:\n",
    "                    if tmp_dict.get(i[0],-1)==-1 and tmp_dict.get(i[1],-1)==-1:\n",
    "                        tmp_dict[i[0]] = type_index\n",
    "                        tmp_dict[i[1]] = type_index\n",
    "                        type_index += 1\n",
    "                    elif tmp_dict.get(i[0],-1)==-1 and tmp_dict.get(i[1],-1)!=-1:\n",
    "                        tmp_dict[i[0]] = tmp_dict[i[1]]\n",
    "                    elif tmp_dict.get(i[0],-1)!=-1 and tmp_dict.get(i[1],-1)==-1:\n",
    "                        tmp_dict[i[1]] = tmp_dict[i[0]]\n",
    "                else:\n",
    "                    tmp_dict[i[0]] = type_index\n",
    "                    type_index += 1\n",
    "            return tmp_dict\n",
    "        def vec_sum(vector_a,vector_b):\n",
    "            result = []\n",
    "            for i in range(len(vector_a)):\n",
    "                result.append(vector_a[i]+vector_b[i])\n",
    "            return result\n",
    "\n",
    "        def vec_divsion(vector,n):\n",
    "            \"\"\"\n",
    "                向量除以一个数\n",
    "            \"\"\"\n",
    "\n",
    "            result = []\n",
    "            for i in vector:\n",
    "                result.append(i/n)\n",
    "            return result\n",
    "\n",
    "\n",
    "\n",
    "        def merge_sim_type(res,tfidf_vec,threshold=0.01):\n",
    "            \"\"\"\n",
    "                param res:原始个文章对应的各个类列表\n",
    "                param res:各个文章的tf_idf的向量化形式\n",
    "                param threshold:类间合并的阈值，如果两个类间余弦距离小于这个值，那么将两个类进行合并\n",
    "            \"\"\"\n",
    "            type_nums = len(set(res))\n",
    "\n",
    "            type_dict = {}\n",
    "            for index,type_num in enumerate(res):\n",
    "                if type_dict.get(type_num,-1)==-1:\n",
    "                    type_dict[type_num] = [index]\n",
    "                else:\n",
    "                    type_dict[type_num].append(index)\n",
    "            center_vec_list = []\n",
    "\n",
    "            for l in type_dict:\n",
    "                sum_vec = tfidf_vec[type_dict[l][0]]\n",
    "                for i in range(1,len(type_dict[l])):\n",
    "                    sum_vec = vec_sum(sum_vec,tfidf_vec[type_dict[l][i]])\n",
    "                center_vec = vec_divsion(sum_vec,len(type_dict[l]))\n",
    "                center_vec_list.append(center_vec)\n",
    "\n",
    "\n",
    "\n",
    "            tmp_dict = dict(sorted(get_result(get_max_sim(get_sim(center_vec_list)),threshold).items(),key=lambda x:x[0]) )\n",
    "            result = []\n",
    "            for i in res:\n",
    "                result.append(tmp_dict[i])\n",
    "            return result\n",
    "  \n",
    "        \n",
    "        \n",
    "        dealed_texts = get_final_split_result(text)\n",
    "        doc_list = get_doc_list(dealed_texts)\n",
    "        idf_dic = train_idf(doc_list)\n",
    "        words = sorted(list(idf_dic))\n",
    "        tfidf = TfIdf(idf_dic,1,doc_list[1],words=words)\n",
    "\n",
    "        tfidf_vec = []\n",
    "        for i,doc in enumerate(doc_list):\n",
    "            tfidf = TfIdf(idf_dic,1,doc,words)\n",
    "            tfidf_vec.append(tfidf.get_tfidf())\n",
    "        print(get_max_sim(get_sim(tfidf_vec)))\n",
    "        tmp_list = sorted(get_result(get_max_sim(get_sim(tfidf_vec)),0.01).items(),key=lambda x:x[0])\n",
    "        res = []\n",
    "        for i in tmp_list:\n",
    "            res.append(i[1])\n",
    "        print(res)\n",
    "        res = merge_sim_type(res,tfidf_vec,0.1) \n",
    "            \n",
    "        \n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = Solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/FT_Camp_3/demo.txt\",'r') as file_object:\n",
    "    text = file_object.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol.classification(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下面结果显示，代标点处理的明显比不代标点处理的更加健壮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4, 0.07169092213618052), (1, 4, 0.21689965350039903), (2, 3, 0.06143311370464855), (3, 2, 0.06143311370464855), (4, 1, 0.21689965350039903), (5, 6, 0.33689683693045686), (6, 5, 0.33689683693045686), (7, 8, 0.15980380121900034), (8, 5, 0.22527703131860338), (9, 13, 0.24320841647821173), (10, 13, 0.16840290905413594), (11, 10, 0.04523365541250764), (12, 13, 0.12081042185695365), (13, 9, 0.24320841647821173)]\n",
      "[0, 0, 1, 1, 0, 2, 2, 3, 3, 4, 4, 4, 4, 4]\n",
      "[(0, 4, 0.046950476039457), (1, 4, 0.06159016543325123), (2, 3, 0.15868339899406816), (3, 2, 0.15868339899406816), (4, 1, 0.06159016543325123)] 0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 0, 2, 2, 2, 2, 3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#代标点处理的\n",
    "sol.classification(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4, 0.021063494879878807), (1, 4, 0.19573382534264042), (2, 3, 0.07160241908674464), (3, 2, 0.07160241908674464), (4, 1, 0.19573382534264042), (5, 6, 0.30693935669029493), (6, 5, 0.30693935669029493), (7, 8, 0.10090384300176322), (8, 5, 0.1759644324963428), (9, 13, 0.2363042212724885), (10, 13, 0.1486233763397309), (11, 10, 0.03689004188633332), (12, 13, 0.11133141100452632), (13, 9, 0.2363042212724885)]\n",
      "[0, 0, 1, 1, 0, 2, 2, 3, 3, 4, 4, 4, 4, 4]\n",
      "[(0, 4, 0.012318882862987041), (1, 4, 0.06373880320399755), (2, 3, 0.14485032908669776), (3, 2, 0.14485032908669776), (4, 1, 0.06373880320399755)] 0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 0, 2, 2, 2, 2, 3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#不代标点处理\n",
    "sol.classification(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 0, 2, 2, 3, 3, 4, 4, 4, 4, 4]\n",
      "[(0, 4, 0.046950476039457), (1, 4, 0.06159016543325123), (2, 3, 0.15868339899406816), (3, 2, 0.15868339899406816), (4, 1, 0.06159016543325123)] 0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 0, 2, 2, 2, 2, 3, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol.classification(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/FT_Camp_3/demo.txt\",'r') as file_object:\n",
    "            ori_text = file_object.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yhk/env3py/jupyter_program/nlp/招行题目三'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
